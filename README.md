# Genetic-Algorithm-Based-Hyperparameter-Optimization-Framework-for-Machine-Learning-
Recent AI advancements have given rise to complex models like ChatGPT  but optimizing them is costly. My project uses genetic algorithms to  systematically explore hyperparameter configurations and discover  optimal settings. Innovative approach Surrogate models further cut  expenses, making it accessible to resource-limited organizations.
![Screenshot 2024-02-03 231817](https://github.com/JokerChew/Genetic-Algorithm-Based-Hyperparameter-Optimization-Framework-for-Machine-Learning-/assets/64265919/7fa8bdf6-a01e-4d35-af32-1abd41a3a455)
In this architecture, the genetic algorithm will be used to perform hyperparameter tuning to obtain the near-optimal or optimal hyperparameters that best fit given diabetes data. During the evaluation process, fitness evaluation for each chromosome will be split into two sections, real simulations, and surrogate modelling to relieve the stress of computational cost. Once the best hyperparameters are obtained, it will be used to train the model to produce a final model with the best hyperparameters, which user will be used to perform predictions to classify diabetes in the system. 

The genetic algorithm follows and mimics the nature of evolution as shown in figure 4. It starts with randomly generated values (populations), with each row representing hyperparameter values. After that, each set of hyperparameters is evaluated according to the accuracy score of the XGBOOST model. The user-defined number of elite candidates will then be chosen for propagation, ideally leading to a generation that is superior to earlier generations via mutation and crossover. Multivariate operators, such as Gaussian and uniform mutation, will be implemented to ensure diversification and avoid stagnation. Also, various crossover techniques will be employed in various generations to provide variation to the genetic algorithm's search strategy. As an illustration, the front crossover technique will be utilized in the first generation, the second generation will use the other half end of the hyperparameters crossover approach, and subsequent generations will alternate between the two methods. Furthermore, the elitism approach also be utilized to preserve the top 3 highest scores of fitness chromosomes for the next generation to better achieve convergence of optimal solutions due to able to continue exploiting promising areas (Tani et al., 2021). In addition, a technique to enable certain numbers of the lowest or worst performance of chromosomes will be removed before undergoing any operators called culling will be utilized. The iterations will be terminated if the maximum iterations are reached or reach a satisfactory result. A genetic algorithm can converge to a global optimum to obtain the best hyperparameter values from a large space, which is time-consuming, long computation time, and computationally expensive for a grid search. 
Lastly, a novel solution called surrogate model to deal with expensive evaluations of fitness function for each solution in each iteration will be proposed. In other words, the simplified surrogate model will be used to speed up the optimization process of genetic algorithm by approximating the complex behaviour of evolutionary process in hyperparameter tuning (Alizadeh et al., 2020). The surrogate model will be utilized to replace the real expensive simulation model to predict the performance of hyperparameters (fitness scores), instead of directly evaluating all objective functions for different hyperparameter combinations (evaluating only a subset of solutions).
